---
title: "Air Quality COVID - Data Loading"
author: "Justin Williams"
date: "5/7/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages}
library(tidyverse)
library(dotenv)
library(RSocrata)
library(RAQSAPI)
library(jsonlite)
library(rlist)
```


## Load data

Load data from API's.

COVID CDC Data

This actually is just a total cumulative sum with a start and end date. NOT a time series with each date. Will have to look for another dataset. May need to get data elsewhere.

```{r covid-api-call}
# load up hidden api key
load_dot_env()

# import datasets to R
covid_df <- read.socrata("https://data.cdc.gov/resource/kn79-hsxy.json",
             app_token = Sys.getenv("SOCRATA_API"))

# save df to data folder
write_csv(covid_df, paste("./data/us_covid_cases_and_deaths_county_",Sys.Date(),".csv", sep = ""))
```

Other data sources.
Columns I would need would be the following:
  - city, county?
  - cum cases, new cases, deaths
  - date (are there any dates with NA?)

```{r other-covid-data}
nyc_covid <- "https://data.cityofnewyork.us/resource/rc75-m7u3.json"
chicago_covid <- "https://data.cityofchicago.org/resource/naz8-j4nc.json"
la_covid <- "https://data.lacity.org/resource/jsff-uc6b.json"
```

Let's look at nyc data

```{r nyc-covid}
nyc_covid_data <- read.socrata(nyc_covid,
             app_token = Sys.getenv("SOCRATA_API"))

# select only date, case, death
nyc_simple <- nyc_covid_data %>% 
  select(date_of_interest, case_count, death_count) %>% 
  mutate(city = "NYC") %>% 
  rename(date = date_of_interest, 
         cases = case_count, 
         deaths = death_count)

saveRDS(nyc_simple,
        file = "./data/nyc_simple.rds")

min(nyc_simple$date)
```

Chicago COVID data

```{r Chicago-COVID}
chicago_covid_data <- read.socrata(chicago_covid,
                                   app_token = Sys.getenv("SOCRATA_API"))
# select only date, case, death
chicago_simple <- chicago_covid_data %>% 
  select(lab_report_date, cases_total, deaths_total) %>% 
  mutate(city = "Chicago") %>% 
  rename(date = lab_report_date,
         cases = cases_total,
         deaths = deaths_total)

saveRDS(chicago_simple,
        file = "./data/chicago_simple.rds")

chicago_simple %>% 
  arrange(date) %>% 
  head(1)
```

LA COVID Data

```{r LA-COVID-data}
la_covid_data <- read.socrata(la_covid,
                              app_token = Sys.getenv("SOCRATA_API"))

la_simple <- la_covid_data %>% 
  select(date, cases, deaths) %>% 
  mutate(city = "LA")

saveRDS(la_simple,
        file = "./data/la_simple.rds")

min(la_simple$date)
```

Now that we have simple datasets of each city, may need to join and create separate columns for cases and deaths in each city, that is if i want a unique row for each date, which im pretty sure needs to be the case for time series data. Or, I could keep them separate to joing with each Air quality dataset (on date). Or I could join them and also keep them separate to join with Air quality for analysis in that regard.

MIN dates of each NYC = 2/29, Chi = 3/1 and LA 3/24. Could just start in MArch 2020 and say NO DATA for Chi or LA, or could Bfill. Or just start the time series analysis 3/24.

Air quality API call

```{r aqs-api-call}
aqs_credentials(username = "justinmorganwilliams@gmail.com",
                key = Sys.getenv("AQS_API"))
```

Get state and county use NYC as proxy, will most likely do LA, Chicago and New York.

```{r get-fips-codes, warning=FALSE}
# get ny state fips
state_fips <- aqs_states()
ny_fips <- state_fips[state_fips$state == "New York",]$stateFIPS

# set county fips df
ny_county_fips <- aqs_counties_by_state(ny_fips)

# create list of county names in nyc
nyc_county_names = c("Bronx", "Kings", "New York", "Queens", "Richmond")

# get nyc county fips
nyc_county_fips <- as.list(ny_county_fips[ny_county_fips$county_name %in% nyc_county_names,]$county_code)


# # get sites by county
# # may need to make this a function
# 
# # set length of vector
# nyc_county_sites <- vector("character", length(nyc_county_fips))
# 
# # loop through county_fips
# for (i in seq_along(nyc_county_fips)) {
#   nyc_county_sites[i] <- aqs_sites_by_county(
#     stateFIPS = ny_fips,
#     countycode = nyc_county_fips[[i]]
#   )
# }
```

Get parameter codes for pollutants.

```{r parameter-codes}
parm_json <- fromJSON("https://aqs.epa.gov/data/api/list/parametersByClass?email=test@aqs.api&key=test&pc=CRITERIA", flatten = T)

parm_json$Data
```

Look at sample data per county.

```{r sample-data-county}
aqs_sampledata_by_county(
  parameter = "88101",
  bdate = as.Date("20200301",
                  format = "%Y%m%d"),
  edate = as.Date("20200331",
                  format = "%Y%m%d"),
  stateFIPS = ny_fips,
  countycode = nyc_county_fips[[1]]
)

# set up sample vector for iteration
nyc_county_pm2.5 <- vector("character", length(nyc_county_fips))

# need to write a function to iterate over this and 
# make it one big dataframe
# maybe use rbind in a function?

for (i in seq_along(nyc_county_fips)) {
  nyc_county_pm2.5 <- 
    aqs_sampledata_by_county(
      parameter = "88101",
      bdate = as.Date("20200301",
                  format = "%Y%m%d"),
      edate = as.Date("20200331",
                  format = "%Y%m%d"),
      stateFIPS = ny_fips,
      countycode = nyc_county_fips[[i]])
}

nyc_county_pm2.5

# try it with mapply
m_test <- mapply(aqs_sampledata_by_county,
       countycode = nyc_county_fips,
       MoreArgs = list(parameter = "88101",
       bdate = as.Date("20200301",
                  format = "%Y%m%d"),
       edate = as.Date("20200331",
                  format = "%Y%m%d"),
       stateFIPS = ny_fips),
       SIMPLIFY = F)

# ok the above makes a list with dataframes in it

# try this with map purr
# this works perfectly, flattening the list with map_dfr and list.flatten
nyc_county_sample_data <- map(.x = nyc_county_fips,
     aqs_sampledata_by_county,
     parameter = "88101",
       bdate = as.Date("20200301",
                  format = "%Y%m%d"),
       edate = as.Date("20200331",
                  format = "%Y%m%d"),
       stateFIPS = ny_fips) %>% 
  map_dfr(list.flatten)

nyc_county_sample_data
```


Ok, now that I have FIPS codes, let's load up some data. I will start by looking at 

## Overview

Research Question

Is there a significant difference in rise of COVID cases and fall of air pollutants (either just PM2.5 or NO2).
Do rise in COVID cases correlate with increased air quality?

Some stuff to look at:
  - Maybe include a table of pollutants, parameter and description?
  - Aggregate all hourly to daily or 24 hour, don't want to go less then 24 hours
  - What is the difference in `method_code`? Is there a data dictionary somewhere?
  - Geospatial map of all stations?
  - Maybe look a year or two prior to COVID to view diff?
  - Should I work with date local or GMT?
  
Seasonality
  - Add year, month, day columns to df so can aggregate as such
  - Format date column as datetime
  - Line charts with plot function, can have multiple on chart. Maybe this would make sense with multiple chemicals in air quality? I wonder if I can import a dataframe with more then one parameter from the API, if so maybe sum them all together to get a total figure and have different variables to compare?
  - Box plots grouped by year, month, day of different pollutants? Seasonality, yearly, monthly, weekly?
  - summary, colSums for na?
  - can have multiple plots on one par(mfrm = c())
  - is it additive or multiplicative

Frequency
  - Frequency select min by daily, monthly, yearly if anything missing need to forward fill or backward fill missing data. Fill function in tidyr
  - should i say that each station should have data for each day?
  
Trends
  - Rolling means smooth time series by averaging out variations at frequencies
  - What parameter to take a rolling mean of?
  - Use **zoo** package to get rolling mean, calculate how many days you want, first arrange desc by year, then group by year then mutate then ungroup which will create a rolling average
  - Can also do yearly, half-yearly, quartley etc...
  - add plot for then add rolling mean as point in base R, can add multiple rolling means can use lines too, add legend
  - 7 day smooths out weekly seasonality but maintains yearly seasonality
  - yearly rolling mean shows long term trend
  - Is it additive or multiplicative?

COVID Specific
  - pivot longer so that date is in column
  - mutate with lubridate to mdy(), group_by county, date, summarise deaths or cases row
  the ungroup
  - this may cumulative total, to change that and look at new cases can use lag function.
  - to use lag function arrange by date, then group by county then mutate by subtracting cumsum - lag(cumsum, default = 0)
  - change scale_x_date, can facet wrap by county with scales = "free_y"
  - trends can effect stationarity, can lead to underestimating future observations.
  - a dataset is stationary once you remove the trend
  - use ma() moving average function to get trend, can plot on top of data with plot
  - subtract data from trend, left with seasonal and random (if additive model)
  
Decompose
  - ts() function?
  - decompose? plot(decompose(df))
    - breaks into seasonality, trend, observed, random (residuals)
    - additive (seasonal+trend+random) 
    - multiplicative (seasonsalxtrendxrandom) for exponential growth
    - pattern in random? indicative of a time series thats not additive
    - set decompose function to specific object
      - $seasonal, $trend (using moving average functionality), $random
      
Comparing different time series
  - ARIMA model which would describe each time series
  - Compare with each pair of models with F-Test:
    - NYC covid to NYC air quality
    - Chi covid to Chi air quality
    - LA covid to LA air quality
    
Log
  - counts number of zeros log 10 = 1 log 100 = 10
  - taking log of something makes multiplicative makes it additive
  multiplying on the scale of the number of zeros, becomes additive
  - multiplying 10 means adding 1 on the log scale
  - software don't really multiplicate model, if it is multiplicative should use a logarhythmic scale
  
Libraries
  - data.table for converting data to time series format
  - fpp2 examing seasonality graphically
  - forecast various functions related to time series
  - stats for applying tests acf, Ljung-Box Tests
  - tseries for applying Dickey Fuller test
  
Formula
  - Yt = Tt x St x Rt

Step 1 TS Objects
  - make time series object with `ts()`
  - use nyc_simple[,2] to make ts object frequency = 365? or month or week?
  - can't have missing data, have to do something with it bfill ffill
  - as trend increases fluctuations are also increasing, this is indicative of multiplicative seasonality.
  
Step 2 Basic Plots
  - plot with base R, type = "l"
  - ggseaonsalplot split on on year, year.label = T? polar? if it is perfect circle no seasonality
  if circles shift in same way, suggests regular seasonality
  - monthplot() lose all sense of annual trend looks at ts object column wise
  
Step 3 Decomposition
  - decompose() type equal too additive or multiplicative
  - random is after divide by trend, divide by seasonality, this is the error left
  should have no trend, no seasonality, it should look random
  - seasonality will always be the same, is a dimensionaless variable
  - plot deomcpose will give 4 plots in one
  - forecast problem becomes simply forecasting trend. multiply trend by seasonality
  randomness (factor) is ignored for forecasting
  

  

  
  

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
