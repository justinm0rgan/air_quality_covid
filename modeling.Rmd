---
title: "Modeling"
author: "Justin Williams"
date: "5/14/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-apckages, warning=FALSE, message=FALSE, include=FALSE}
library(tidyverse)
library(lubridate)
library(prophet)
library(forecast)
library(tseries)
library(reticulate)
```


## Modeling

This notebook will deal exclusively with modeling of the time series data. 

  - dickey-fuller test to confirm non-stationarity or stationariy
    - null = data is not stationary
    - alt = data is stationary
    - for data to be stationary ADF test should have p-value <= sig level
  - Random Drift?
  - ARIMA model https://github.com/MGCodesandStats/energy-weather-modelling/blob/master/arima-weather-dublin-airport.R
    - plot diagnostics
    - using gridsearch https://towardsdatascience.com/setting-arima-model-parameters-in-r-grid-search-vs-auto-arima-19055aacafdf
  - SARIMA model
    - plot diagnostics
  - SARIMAX?
  - Prophet
    - prophet_plot_compenents
    - customizing holidays and events? maybe day of lockdown? and anomalous day?
    - removing outliers?
    
## Load Time Series Object

```{r load-ts-object}
ts_nyc <- readRDS(file = "./data/ts_nyc.rds")
simple_aqs <- readRDS(file = "./data/simple_yearly/simple_aqs.rds")
```

Let's look at splitting off the first 3 years as train and last 2 as test

```{r train-test}
train_ts_nyc <- window(ts_nyc, start = decimal_date(as.Date("2017-01-01")), 
                         end =decimal_date(as.Date("2019-12-31")))
test_ts_nyc <- window(ts_nyc, start = decimal_date(as.Date("2020-01-01")), 
                        end = decimal_date(as.Date("2021-12-31")))
```

View train/test split.

```{r autoplot-train-test}
cust = c("#A63446","#0C6291")

autoplot(ts_nyc) +
  autolayer(train_ts_nyc, series = "Train") +
  autolayer(test_ts_nyc,
            series = "Test") +
  guides(color = guide_legend(title = "")) +
  scale_color_manual(values = cust) +
  labs(x = "", y = "PM2.5 (ug/m3)", 
         title = "Train/Test Mean PM2.5 per Month") +
  theme_minimal() +
  theme(plot.title.position = "plot",
        plot.title = element_text(hjust = 0.5))
```

### Random Walk

Try with random walk.
First have to decompose seasonally with Loess.
Choose 6 months as seasonality trend.

```{r random-walk}
# decompose seasonally 
train_stl <- stl(train_ts_nyc, s.window = 180)
nyc_forecast <- forecast(train_stl, 
                         method = "rwdrift", 
                         h = 730)
plot(nyc_forecast)
```

This is pretty bad, obviously need to do some transformations and statistical tests before I can forecast.

## Python Modeling

Try working with time series data in python instead of R.

```{python import_libraries, include=FALSE}
import pandas as pd
import numpy as np
import math
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from matplotlib.pylab import rcParams
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.tsa.arima_model import ARIMA
```

Import python script with modeling functions.

```{r import-python-functions}
source_python("./src/modeling_functions.py")
```

Convert data frame to pandas df.

```{python convert-to-pandas}
df = r.simple_aqs
df.head()
```

### Inspect Stationarity

```{python inspect-stationarity}
# set rolling mean
roll_mean = df["mean_measurement"].rolling(window=30, center=False).mean()

# plot
plot_time_series(df_col1=df["mean_measurement"], series2=roll_mean,
                label1="Observed", label2="Rolling Mean",
                plot_title='Monthly Rolling average PM2.5 by Year')
```

Current data contains definite seasonality and slight upward trend.
There is a definite downtrend once 2020 hits, then an uptick in 2021.

### Dickey Fuller Test

```{python dickey-fuller-test}
dickey_fuller(df["mean_measurement"])
```

P-value is pretty high (above .05) which indicates the data is NOT stationary. Need to remove seasonality and trend prior to forecast. 

### Remove trends

Explore different methods for removing trends and seasonality such as, differencing, rolling means and various transformations. 

Let's try log transformation.

### Log Transformation

One way to enforce stationarity can be to log transform the data, which penalizes higher values more then lower.

```{python log-transform}
# there is one negative value, ignore error
old = np.seterr(invalid='ignore')

log_roll_mean = np.log(df["mean_measurement"]).rolling(window=30, center=False).mean()
log_data = np.log(df["mean_measurement"])

plot_time_series(df_col1=log_data, series2=log_roll_mean,
                label1="Observed", label2="Log Rolling Mean",
                plot_title='Monthly Rolling average PM2.5 by Year (Log Transformed)')
```

### Check Stationarity

```{python stationarity-check}
log_data.dropna(inplace=True)
dickey_fuller(log_data)
```

Definitely looks stationary now with a p-value of 0.000003.

### Subtract Rolling Mean

```{python subtract-rolling-mean}
roll_mean = df["mean_measurement"].rolling(window=30, center=False).mean()
data_minus_roll_mean = df["mean_measurement"] - roll_mean
data_minus_roll_mean.head(30)
```

Drop missing values

```{python drop-missing-rolling-mean}
# Drop the missing values from time series calculated above
data_minus_roll_mean.dropna(inplace=True)
```

Plot rolling mean.

```{python rolling-mean-plot}
# take rolling mean
roll_mean = data_minus_roll_mean.rolling(window=30, center=False).mean()

# create plot
plot_time_series(df_col1=data_minus_roll_mean, series2=roll_mean,
                label1="Observed", label2="Rolling Mean (Monthly)",
                plot_title='Monthly Rolling average PM2.5 by Year (Minus Rolling Mean)')
```

### Check stationarity

```{python stationarity-rolling-mean}
dickey_fuller(data_minus_roll_mean)
```

Very high p-value, definetly won't use this one.

### Differencing

Another method for dealing with trend and seasonality is differencing. We take the difference of an observation at a particular moment with that at the previous instant (lag).

First-order differencing can be done in Pandas using `diff()` method, periods denotes 1 period lag.

```{python differencing}
# 1 period lag
data_diff = df["mean_measurement"].diff(periods=1)

# drop missing values
data_diff.dropna(inplace = True)
```

Let's plot.

```{python differencing_plot}
# get rolling mean of lag
roll_mean_lag = data_diff.rolling(window=30, center=False).mean()

# create plot
plot_time_series(df_col1=data_diff, series2=roll_mean_lag,
                label1="Observed", label2="Rolling Mean",
                plot_title='Monthly Rolling average PM2.5 by Year (Differenced)')
```

### Check Stationarity

```{python dickey-fuller-differenced}
dickey_fuller(data_diff)
```

This is a very small p-value, looks even smaller then the log transformation.
Maybe we will go with this one?

Let's decompose with Python and see how that looks.

LET"S SPLIT DATA HERE

### Autocorrelation

Autocorrelation refers to how correlated a time series is with its past values. Processes with greater autocorrelation are more predictable than those without any form of autocorrelation. This process compares each value in the time series with it's previous value. This is called "lag 1 autocorrelation".


```{python}
# shift the series forward by 3 lag
total = df['mean_measurement']
total_shift_30 = total.shift(periods=30)
lag_30 = pd.concat([total_shift_30, total], axis=1)
lag_30.corr()
```

Let's plot!

```{python autocorrelation-1-lag}
lag_30.plot()
plt.show()
```

Highest correlation was 365 days (1 year) but 30 days was second highest.
These are not high $R$ values. Which suggests it is not a very predictable pattern.

### Autocorrelation Function (ACF)

The autocorrelation function (ACF) is the plot used to see the correlation between the points, up to and including the lag unit.

  - It's a function that represents autocorrelation of a time series as a function of the time lag.
  - lets you know how the given time series is correlated with itself.
  - The dotted lines in the plot tell you about the statistical significance of the correlation.
  
```{python plot-acf}
rcParams['figure.figsize'] = 14, 5

plot_acf(data_diff, lags=30);
plt.show()

help(plot_acf)

```

There are a few significant points. The earlier lags being the most sigificant.

Differenced data seems a bit more stable. 

### Partial Autocorrelation Function (PACF)

PACF is a subset of ACF. PACF expresses the correlation between observations made at two points in time while accounting for any influence from other data points.(unlike the autocorrelation function, which does not control for other lags).

  - PACF can be interpreted as a regression of the series against its past lags.
  
```{python pacf}
plot_pacf(data_diff, lags=30);
plt.show()
```

Any lag outside of confidence interval (blue shaded area) is significant.
Looks like there are more significant ones then not. 

Ok, next will go into modeling, need to split data first. 

### ARIMA Model


